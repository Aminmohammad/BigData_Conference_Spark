{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Interactive Analysis with Spark\n",
    "\n",
    "## Table of Content\n",
    "1. [Initialization](#1.-Initialization)\n",
    "2. [Creating a RDD](#2.-Creating-a-RDD)\n",
    "3. [Getting Help](#3.-Getting-Help)\n",
    "4. [Action on a Dataset](#4.-Action-on-a-Dataset)\n",
    "5. [Dataset Transformation](#5.-Dataset-Transformation)\n",
    "6. [Caching a Dataset](#6.-Caching-a-Dataset)\n",
    "7. [Filtering a Dataset](#7.-Filtering-a-Dataset)\n",
    "8. [Reduction Operation](#8.-Reduction-Operation)  \n",
    "  8.1 [Transforming in Key-Value Pairs](#8.1-Transforming-in-Key-Value-Pairs)  \n",
    "  8.2 [Filtering Unrelated Entries](#8.2-Filtering-Unrelated-Entries)  \n",
    "  8.3 [Aggregating the Results by Key](#8.3-Aggregating-the-Results-by-Key)  \n",
    "  8.4 [Computing the Most Frequent Languages](#8.4-Computing-the-Most-Frequent-Languages)    \n",
    "  8.5 [Bar Chart](#8.5.-Bar-Chart)\n",
    "9. [Ending the Analysis](#9.-Ending-the-Analysis)\n",
    "10. [Recap](#10.-Recap)\n",
    "\n",
    "\n",
    "## List of Exercises\n",
    "1. [Exercise 1: How to Cache?](#Exercise-1)\n",
    "2. [Exercise 2: How to Peek?](#Exercise-2)\n",
    "3. [Exercise 3: How to Filter?](#Exercise-3)\n",
    "4. [Exercise 4: How to Sort?](#Exercise-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "\n",
    "In this notebook, we will use Spark to analyze brievly a simple structured dataset.\n",
    "\n",
    "First, we need to import Spark's Python module named `pyspark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import spark_helper # homemade module to ease the workshop progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = pyspark.SparkContext()\n",
    "except ValueError:\n",
    "    print(\"Warning : a SparkContext already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we execute Spark locally, the context creation can be used to Spark. It is also possible to launch Spark and the Python interpreter / notebook using the script named `pyspark`. In this case, the context will already be created (under the name `sc`) and a `ValueError` exception is raised when we try to create a second context. The exception has not effect so we simply catch it and display a warning message.\n",
    "\n",
    "We can consult the dashboard of our Spark application at the link computed by the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Spark app's dashboard link: {}\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a RDD from text files representing the page visits data of Wikipedia. This data should be in the folder `data/pagecounts`. \n",
    "\n",
    "The notebook [0-Configuration.ipynb](0-Configuration.ipynb) can assist you in downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts = sc.textFile('data/pagecounts/*.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [pagecounts](https://dumps.wikimedia.org/other/pagecounts-raw/) file looks like this  \n",
    "```\n",
    "af Spesiaal:Onlangse_wysigings 3 101681\n",
    "af Spesiaal:RecentChanges 2 2248\n",
    "af Suid-Afrika 1 30698\n",
    "af Tuisblad 14 155257 \n",
    "af Varkgriep 4 42236\n",
    "af Wikipedia 2 32796\n",
    "```\n",
    "\n",
    "It is a tabular file, where each line is a distinct entry and the columns represent\n",
    "2. the project name (language);\n",
    "3. the page title;\n",
    "4. the number of requests;\n",
    "5. the size of the content returned.\n",
    "\n",
    "We can look at a few entries with the RDD's method `take` to get the first `K` elements of the dataset. Here, `K = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first10 = pagecounts.take(10)\n",
    "print(first10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since `take` returns a list, we can iterate on the result and print it \"prettily\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in first10:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting Help\n",
    "\n",
    "At any moment, you can get help on a Python object using the `help()` function. For example, if we want to know more aboud the RDD's `take()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(pagecounts.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Action on a Dataset\n",
    "\n",
    "The `take()` method is one among multiple available *actions* we apply on a RDD. An exhaustive list of action is available at the following URL:\n",
    "https://spark.apache.org/docs/latest/programming-guide.html#actions\n",
    "\n",
    "In case where we do not want to leave the notebook tab, we can call `help()` directly on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(pagecounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the available action, lets take the method `count()`. What does it return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action apply on a RDD leads to the creation of one or many task and the production of a result. Every task executed in the same app can be visualised in the Spark's dashboard. In this interface, we can track the progress of a task, and check different performance measures on the task, for example its duration and cache statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the 10 first element of our dataset that we retrieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realise that the RDD is composed of each line of input text files, but that is not possible to access to individual column. **Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first1 = pagecounts.first()\n",
    "first1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action `first()` as its name states, return the first entry of the dataset. We see that each entry is a single string. We will need to transform that first RDD in a second in order to divide each string in a list of 4 elements. To do this, we will use the method `split()` of Python string object.\n",
    "\n",
    "We first test it on the dataset first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str.split(first1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to apply this transformation to every RDD's entry. The RDD's method `map(func)` returns a new distributed dataset formed by passing each element of the source through a function *func*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tab = pagecounts.map(str.split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of this transformation is *lazy*. Spark does not compute anything as long as a result is not requested by an action. To convince yourself, execute the preceding cell, then visit the Spark dashboard. You should see that no job have been added to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Spark app's dashboard link: {}/jobs\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caching a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we expect to operate frequently on the same dataset, it can be useful to tell Spark to keep it in memory.\n",
    "\n",
    "To do so, we use the `cache()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tab.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDDs stored in memory are displayed in the **Storage** section of Spark web interface. Note that datasets are not loaded in memory until an action is called on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Spark app's dashboard link: {}/storage\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To free memory used by cached RDD that we no longer need, we need to call the `unpersist()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tab.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtering a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have a RDD that is easier to manipulate, we can star the analysis. First, we take interest in page s rom Wikipedia English. The following line filter the last RDD we created , eepsing nly the entries written in English.\n",
    "\n",
    "Try to answer the following quiz before executing the cell:  \n",
    "* What sort of argument takes the `filter()` method?\n",
    "* What type is `x`?\n",
    "* Is filter an action or a transformation?\n",
    "* What does `filter()` return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pagecounts_en = pagecounts_tab.filter(lambda x: x[0] == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "**Write the code to cache the new RDD in the following cell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now count the number of pages in English. The macro notebook `%time` will indicate how long it took Spark to count the number of entries in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pagecounts_en.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we told Spark to keep the dataset in memory, the time required to count the number of pages should be shorter for the second execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pagecounts_en.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we committed an action on a cached RDD, it should now figure in the **Storage** section of our app's dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Spark app's dashboard link: {}/storage\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reduction Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in producing a bar chart of the number of pages request per language in our dataset. In order to do this, we will need to aggregate the number of requested pages for each language. This type of operation is called a *reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Transforming in Key-Value Pairs\n",
    "\n",
    "First, we transform our dataset to only keep the language and the number of requests. Furthermore, the number of request field is a string, we therefore use `int()` to convert it in an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tuple = pagecounts_tab.map(lambda entry: (entry[0], int(entry[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "**Take a look at the first 5 elements of the new RDD to confirm the transformation is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides functions to work with key-value pairs. In our new dataset, the key is the language and the number of requests the value. The `key()` method of RDD returns a new RDD composed only of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tuple.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `values()` method is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tuple.values().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Filtering Unrelated Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed, the key of our RDD sometimes has the form \"xx.x\". The project suffix \".x\" corresponds to the project name. The following abbreviations are used:  \n",
    "```\n",
    "wikibooks: \".b\"\n",
    "wiktionary: \".d\"\n",
    "wikimedia: \".m\"\n",
    "wikipedia mobile: \".mw\"\n",
    "wikinews: \".n\"\n",
    "wikiquote: \".q\"\n",
    "wikisource: \".s\"\n",
    "wikiversity: \".v\"\n",
    "mediawiki: \".w\"\n",
    "```\n",
    "Projects without a period and a following character are wikipedia projects. \n",
    "\n",
    "#### Exercise 3\n",
    "**Design a filter to only keep the pages that are Wikipedia projects.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Aggregating the Results by Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the number of pages equested for each language. In order to do this, we use the `reduceByKey()` method. As its name states, this function expect the RDD to be composed of key-value pairs.\n",
    "\n",
    "When called on a dataset of $(K, V)$ pairs, `reduceByKey()` returns a dataset of $(K, V)$ pairs where the values for each key are aggregated using the given reduce function *func*, which must be of type $(V,V) \\rightarrow V$.\n",
    "\n",
    "Since we want the total number of pages er language, our aggregating function will be the addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang_pagecounts = pagecounts_filt.reduceByKey(lambda x, y: x + y).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey()` is a transformation, therefore the result is a new RDD. To visualize the entire content of the latter, we can use the `collect()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang_pagecounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Computing the Most Frequent Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to determine the 5 most frequent languages requested during the period observed. In order to do this, multiple solutions are available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1. Sort locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5 = sorted(lang_pagecounts.collect(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2 Ask Spark to sort the dataset using the `sortBy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5 = lang_pagecounts.sortBy(keyfunc=lambda x: x[1], ascending=False).take(5)\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3 Ask Spark for the top 5 using the `top()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5 = lang_pagecounts.top(5, lambda x: x[1])\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "**Can you think of another method to get the same result?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Bar Chart\n",
    "\n",
    "Since we are in an interactive notebook, we can use Python plotting library `matplotlib` to plot our bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "top5_t = list(zip(*top5))\n",
    "ax.bar(range(len(top5_t[0])), top5_t[1], width=0.35, align=\"center\")\n",
    "ax.set_xticks(range(len(top5_t[0])))\n",
    "ax.set_xticklabels(top5_t[0])\n",
    "ax.set_xlabel(\"lang\")\n",
    "ax.set_ylabel(u\"number of requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ending the Analysis\n",
    "\n",
    "Once the analysis is done, we need to tell Spark to free the resources and destroy the context using the `SparkContext` method `stop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recap\n",
    "\n",
    "In this notebook, we used and learned about the following parts of \n",
    "**[Python Spark API](http://spark.apache.org/docs/latest/api/python/)**:\n",
    "1. Import Spark Python module: \n",
    "**[`import pyspark`](http://spark.apache.org/docs/latest/api/python/pyspark.html)**\n",
    "2. Create a SparkContext:\n",
    "**[`pyspark.SparkContext()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext)**\n",
    "2. Create a RDD from text files:\n",
    "**[`SparkContext.textFile(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)**\n",
    "3. Take a the first *num* elements from a RDD: \n",
    "**[`Rdd.take(num)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take)**\n",
    "3. Count the number of elements in a RDD: \n",
    "**[`Rdd.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count)**\n",
    "4. Retrieve the first element of a RDD: \n",
    "**[`RDD.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first)**\n",
    "5. Apply a transformation on each element of a RDD:\n",
    "**[`RDD.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)**\n",
    "4. Cache a RDD:\n",
    "**[`RDD.cache()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache)**\n",
    "5. Remove a RDD from memory: \n",
    "**[`RDD.unpersist()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.unpersist)**\n",
    "5. Filter a RDD:\n",
    "**[`RDD.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter)**\n",
    "6. Merge the values for each keys: \n",
    "**[`RDD.reduceByKey(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)**\n",
    "7. Get all elements of a RDD: \n",
    "**[`RDD.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect)**\n",
    "8. Sort the elements of a RDD:\n",
    "**[`RDD.sortBy(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy)**\n",
    "9. Get the top $N$ elements from a RDD:\n",
    "**[`RDD.top(N)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top)**\n",
    "10. End the SparkContext:\n",
    "**[`SparkContext.stop()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.stop)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
