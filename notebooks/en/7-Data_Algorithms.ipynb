{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Algorithms\n",
    "\n",
    "## 1. K-Means Clustering\n",
    "\n",
    "### 1.1 Description\n",
    "\n",
    "As stated on Wikipedia\n",
    "> Given a set of observations $(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n)$, where each observation is a *d*-dimensional real vector, $k$-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $\\mathbf{S}= \\{S_1, S_2, \\ldots, S_k\\}$ so as to minimize the within-cluster sum of squares (WCSS) (sum of distance functions of each point in the cluster to the K center). In other words, its objective is to find:\n",
    ">\n",
    ">$\\begin{equation}\\underset{\\mathbf{S}} {\\operatorname{arg\\,min}}  \\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_i} \\left\\| \\mathbf x - \\boldsymbol\\mu_i \\right\\|^2\\end{equation}$\n",
    ">\n",
    ">where $\\boldsymbol\\mu_i$ is the mean of points in $S_i$.\n",
    "\n",
    "\n",
    "### 1.2 Algorithm\n",
    "\n",
    "1. Initialize cluster centroids $\\boldsymbol\\mu_1, \\boldsymbol\\mu_2, \\ldots, \\boldsymbol\\mu_k \\in \\mathbb{R}^n$ \n",
    "2. Repeat until convergence:  \n",
    "  2.1 For every $i$, set $y_i = \\arg\\min \\|\\mathbf{x}_i - \\boldsymbol\\mu_j\\|^2$  \n",
    "  2.2 For each $j$, set $\\boldsymbol\\mu_j = \\frac{\\sum_i^m1\\{y_i = j\\}\\mathbf{x}_i}{}$\n",
    "\n",
    "### 1.3 Objective\n",
    "\n",
    "Write your own version of k-means algorithm using [Spark's RDD API](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 10**3\n",
    "N_FEATURES= 2\n",
    "N_CENTERS = 4\n",
    "STD = 0.5\n",
    "BOX = (-10.0, 10.0)\n",
    "features, tlabels = make_blobs(n_samples=N_SAMPLES, \n",
    "                               n_features=N_FEATURES, \n",
    "                               centers=N_CENTERS, \n",
    "                               cluster_std=STD, \n",
    "                               center_box=BOX)\n",
    "\n",
    "rdd = sc.parallelize(features).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_label(point, centroids):\n",
    "    \"\"\"Return the label of the closest centroid.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "def kmeans(rdd, n_centers, n_iter)\n",
    "    # 0. Compute dataset bounding box\n",
    "    # 1. Initialize cluster centroids\n",
    "    # 2. Repeat until convergence\n",
    "    #   2.1 Compute label for each points\n",
    "    #   2.2 Update centroids\n",
    "    <FILL IN>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_label(point, centroids):\n",
    "    return numpy.argmin([numpy.linalg.norm(point - centroid) for centroid in centroids])\n",
    "\n",
    "def kmeans(rdd, n_centers, n_iter):\n",
    "    centroids = numpy.random.uniform(low=BOX[0], high=BOX[1], size=(n_centers, N_FEATURES))\n",
    "    for i in range(n_iter):\n",
    "        labels = rdd.map(lambda x: compute_label(x, centroids))\n",
    "\n",
    "        centroid_map = labels.zip(rdd)\\\n",
    "                             .mapValues(lambda x: (x, 1))\\\n",
    "                             .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "                             .mapValues(lambda x: x[0] / x[1])\\\n",
    "                             .collectAsMap()\n",
    "        for i in range(n_centers):\n",
    "            centroids[i] = centroid_map.get(i, numpy.random.uniform(low=BOX[0], high=BOX[1], size=(1, N_FEATURES)))\n",
    "    return centroids\n",
    "\n",
    "kmeans(rdd, N_CENTERS, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Compare with MLlib Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "clusters = KMeans.train(rdd, N_CENTERS, maxIterations=10, runs=1, initializationMode=\"random\")\n",
    "clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
