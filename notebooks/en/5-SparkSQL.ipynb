{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Data Analysis with Spark SQL\n",
    "\n",
    "Pour éviter d'avoir à constamment manipuler des jeux de données au format texte, il peut être intéressant de structurer nos données. SparkSQL permet de structurer un jeu de données en définissant un schéma.\n",
    "\n",
    "- On peut connecter Spark SQL sur une base de données externes, par exemple une base de données PostgreSQL.\n",
    "- Le `Dataframe` permet d'obtenir le même niveau de performance peu importe le langage utilisée. Les RDD classiques sont normalement plus performant en Java ou en Scala.\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Initialization](#1.-Initialization)  \n",
    "  1.1 [Spark](#1.1-Spark)  \n",
    "  1.2 [Spark SQL](#1.2-Spark-SQL)\n",
    "2. [Dataframe](#2.-Dataframe)  \n",
    "  2.1 [Reading Data](#2.1-Reading-Data)  \n",
    "  2.2 [Structuring Data](#2.2-Structuring-Data)  \n",
    "  2.3 [Creating a Dataframe](#2.3-Creating-a-Dataframe)  \n",
    "  2.4 [Registering a Table](#2.4-Registering-a-Table)  \n",
    "  2.5 [Querying Data](#2.5-Querying-Data)  \n",
    "  2.6 [Aggregating Results](#2.6-Aggregating-Results)\n",
    "3. [Writing Results to Disk](#3.-Writing-Results-to-Disk)\n",
    "4. [Ending Spark SQL Analysis](#4.-Ending-Spark-SQL-Analysis)\n",
    "5. [Recap](#Recap)\n",
    "6. [References](#References)\n",
    "\n",
    "## List of Exercises\n",
    "\n",
    "1. [Exercise 1: Initialize Spark](#Exercise-1)\n",
    "2. [Exercise 2: ](#Exercise-2)\n",
    "3. [Exercise 3: ](#Exercise-3)\n",
    "4. [Exercise 4: ](#Exercise-4)\n",
    "5. [Exercise 5: ](#Exercise-5)\n",
    "6. [Exercise 6: ](#Exercise-6)\n",
    "7. [Exercise 7: ](#Exercise-7)\n",
    "8. [Exercise 8: ](#Exercise-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Spark\n",
    "#### Exercise 1\n",
    "\n",
    "Import the necessary Python module(s) and create a Spark context. \n",
    "\n",
    "**Warning**, verify if there already exist a context and handle possible exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 Spark SQL\n",
    "\n",
    "We can now import the components that we need to analyze structured data from Spark SQL module `pyspark.sql`.\n",
    "* `SQLContext`: Main entry point for Spark SQL functionality. It will be used to create Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataframe\n",
    "\n",
    "Textbook definition:    \n",
    "\n",
    "> A data frame is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case.\n",
    "\n",
    "In Spark, a dataframe is a distributed collection of data grouped into named columns. It is equivalent to a relational table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 3 \n",
    "\n",
    "Create an RDD with the dataset we used previously `data/pagecounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pagecounts = sc.textFile('data/pagecounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Count the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "Transform the previous RDD into a second one where each field originally separated by white spaces are now elements of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pgsplit = pagecounts.map(str.split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exercise 6\n",
    "\n",
    "To validate the transformation, display the first 8 elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pgsplit.take(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "As you can see, the third and fourth fields are numbers represented as text. Transform the RDD in order to convert these strings to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pgsplit = pgsplit.map(lambda x: (x[0], x[1], int(x[2]), int(x[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Structuring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original dataset is strictly text, we now want to give a structure that is define field name and type.\n",
    "\n",
    "To define our structure, we use Spark SQL data types that are defined in [`pyspark.spl.types`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types). We import a small subset of type that we need `LongType` and `StringType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL also provides two types to defines dataframe structure:\n",
    "- `StructType`: Data type representing a row of a dataframe.\n",
    "- `StructField`: Data type representing a field of a row. It is mainly defined by a name and a type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all these classes, we can define our data schema. The order in the list must correspond to the order in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('lang',    StringType()), \n",
    "                     StructField('name',    StringType()), \n",
    "                     StructField('request', LongType()), \n",
    "                     StructField('size',    LongType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Creating a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dataframe, we simply need to invoke the `createDataFrame()` method of our Spark SQL Context and provide it an RDD and our data structure (or schema). \n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "Replace `<FILL IN>` in the following cell by the proper RDD for `data/pagecounts` and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts = sqlCtx.createDataFrame(pgsplit, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then manipulate the dataframe using its [`pyspark.sql.DataFrame` API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "\n",
    "We can, for example, show the first lines of the dataframe as a table in ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe can also be cached in case we plan to do multiple request on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Registering a Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to query our dataframe with SQL, we need to register it as a table in the Spark SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.registerTempTable(\"page_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Querying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are now able to interogate our data using the Structured Query Language (SQL). The following query request the top 10 most requested page in spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlCtx.sql(\"SELECT name, request \"\n",
    "                \"FROM page_table \"\n",
    "                \"WHERE lang='es' \"\n",
    "                \"ORDER BY request DESC \"\n",
    "                \"LIMIT 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A query on a dataframe is a Spark transformation. Therefore, to compute the result, we need to call an action. \n",
    "\n",
    "#### Exercise 8 \n",
    "\n",
    "Call the right method to display the dataframe resulting from the preceding query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 SQL 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose the preceding query in keywords:\n",
    "\n",
    "#### SELECT\n",
    "\n",
    "Indicate which variable we want to collect. The name of variable have been defined when structuring our data in [section 2.2](#2.2-Structuring-Data).\n",
    "\n",
    "#### FROM\n",
    "\n",
    "Indicate the source of data. The name of the table has been defined in [section 2.4](#2.4-Registering-a-table).\n",
    "\n",
    "#### WHERE\n",
    "\n",
    "Filter the entries based on predicates in function of the variables. \n",
    "\n",
    "#### ORDER BY [...] DESC\n",
    "\n",
    "Indicate we wish to order the resulting dataframe in function of a certain variable, in a certain order. \n",
    "\n",
    "#### LIMIT N \n",
    "\n",
    "Return only a subset of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 SQL as an API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.where(\"lang = 'en'\")\\\n",
    "            .select(\"name\", \"request\")\\\n",
    "            .orderBy(\"request\", ascending=False)\\\n",
    "            .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Aggregating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque cependant que certaines page reviennent plusieurs fois dans notre palmarès. La raison est qu'on a omis d'additionner le nombre de vues pour une même page. Il faut effectuer une opération d'aggrégation `GROUP` et `SUM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.sql(\"SELECT name, SUM(pagecount) as sumation \"\n",
    "           \"FROM page_table \"\n",
    "           \"WHERE request>=100 \"\n",
    "           \"AND lang='es' \"\n",
    "           \"GROUP BY name \"\n",
    "           \"ORDER BY sumation DESC \"\n",
    "           \"LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating Using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.where(\"lang = 'en'\")\\\n",
    "            .select(\"name\", \"request\")\\\n",
    "            .groupBy('name')\\\n",
    "            .agg({'request' : 'sum'})\\\n",
    "            .orderBy(\"sum(request)\", ascending=False)\\\n",
    "            .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- **EXERCICE** Pour vous convaincre de l'utilité de SparkSQL pour simplifier l'analyse de données, écrivez le code nécessaire en utilisant les méthode de transformation des RDD de base (`map`, `filter`, `reduce`, etc.) et d'action (`first`, `collect`, `take`) pour produire le même résultat que la requête SQL précédente.\n",
    "\n",
    "Utilisez le RDD que vous avez créez au début du notebook comme point de départ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing Results to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Apache Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "11- Pour éviter d'avoir à restructer nos données à chaque fois, on peut sauvegarder les au format [Apache Parquet](https://parquet.apache.org/). Le format va conserver le schéma et l'ordre des données intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.write.parquet(\"data/pagecounts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reading back the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12- On peut ensuite facilement créer un nouveau `DataFrame` en lisant nos fichiers au format Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecount_parq = sqlCtx.read.parquet(\"data/pagecounts.parquet\")\n",
    "pagecount_parq.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13- Finalement, on arrête le contexte Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ending Spark SQL Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL's context do not need to be terminated prior to leaving the notebook.\n",
    "\n",
    "#### Exercise XXX\n",
    "\n",
    "Terminate the Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Créez un nouveu notebook.\n",
    "1. Créez un nouveau contexte Spark et un contexte SQL.\n",
    "1. Créez un RDD à partir des données d'entrée `data/pagecounts.parquet`.\n",
    "2. Transformez le RDD en un RDD contenant la taille totale des pages vues par langue  \n",
    "    1. À l'aide des méthodes Spark  \n",
    "    2. À l'aide d'une requête SQL\n",
    "3. Limitez le contenu du RDD au 3 langues les moins populaires \n",
    "4. Affichez le résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
