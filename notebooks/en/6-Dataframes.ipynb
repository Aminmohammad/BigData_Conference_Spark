{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 3: Dataframes\n",
    "\n",
    "## 1. Analyzing Transactions\n",
    "\n",
    "This problem is taken from [Data Algorithms: Recipes for Scaling Up with Hadoop and Spark](http://shop.oreilly.com/product/0636920033950.do) by Mahmoud Parsian.\n",
    "\n",
    "Consider a company such as Amazon, which has over 200 million users and can do hundreds of millions of transactions per day. The users data consists of users’ location information and the transactions data includes user identity information, but no direct information about a user’s location. We are interested in finding which countries are our best consumers in terms of average order total cost.\n",
    "\n",
    "### 1.1 Dataset\n",
    "We have access to seven tables of a store's database as tab separated value files (`.tsv`):\n",
    "- customers (`data/store/customers.tsv`)\n",
    "- orders (`data/store/orders.tsv`)\n",
    "- order details (`data/store/order_details.tsv`)\n",
    "- products (`data/store/products.tsv`)\n",
    "- categories (`data/store/categories.tsv`)\n",
    "- shippers (`data/store/shippers.tsv`)\n",
    "- suppliers (`data/store/suppliers.tsv`)\n",
    "\n",
    "The tables can be viewed online: [SQL Tutorial Database](https://docs.google.com/spreadsheets/d/1_rn2PWl5qqw7ZuBuCm6D7FJrHepOlunjpCPbvitdxxQ/edit?usp=sharing).\n",
    "\n",
    "### 1.2 Objective\n",
    "We are interested in finding the average order cost per country.\n",
    "\n",
    "### 1.3 Instructions\n",
    "\n",
    "1. Identify the fields that you will need to find the answer; \n",
    "2. Initialize the Spark and Spark SQL contexts;\n",
    "3. Read the datasets you need as dataframes;\n",
    "4. Join the tables and keep only the fields you need;\n",
    "5. Compute the total cost per order;\n",
    "6. Compute the average order cost per country;\n",
    "7. Display the countries that are spending the most.\n",
    "\n",
    "### 1.4 Hints\n",
    "\n",
    "- If you go through the dataset, you will see that the information required to fulfill the objective is scatter among the different tables.\n",
    "- An order may include multiple products, you will need to aggregate the order details to compute the order total cost.\n",
    "- Some of the methods required in this practice have not been covered so far, you will need to look at the [DataFrame's API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "- If you are stuck peek at the [Recap](#2.-Recap) section to get insights on what functions you should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Data Algorithms: Recipes for Scaling Up with Hadoop and Spark](http://shop.oreilly.com/product/0636920033950.do) by Mahmoud Parsian\n",
    "* [W3C School - Store Dataset](http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
