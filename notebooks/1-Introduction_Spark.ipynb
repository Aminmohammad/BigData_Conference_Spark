{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse interactive\n",
    "## Initialisation\n",
    "Utilisons Spark pour analyser sommairement un jeu de données.\n",
    "\n",
    "On doit dans un premier temps importer le module Python pour Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On doit ensuite créer un contexte Spark. Si l'application Jupyter a été lancé à l'aide de `pyspark`, le contexte est déjà créé, et une exception de type `ValueError` est lancée. Cette exception est décélé et on affiche tout simplement un message d'avertissement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = pyspark.SparkContext()\n",
    "except ValueError:\n",
    "    print(\"Attenttion : Il existe déjà un SparkContext.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on exécute Spark localement, la création du contexte à lancer Spark. Pour s'en convaincre, on peut visiter la console web de Spark: [http://localhost:4040](http://localhost:4040).\n",
    "\n",
    "Si on se trouve sur une grappe de calcul, on peut déterminer le lien exécutant la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "print(\"http://{hostname}:4040\".format(hostname=socket.gethostname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un jeu de données (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant créer un RDD à partir d'un fichier texte contenant les données de visite de Wikipedia. Les données doivent se trouver dans le répertoire `data/pagecounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts = sc.textFile('data/pagecounts/*.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contenu d'un fichier pagecounts ressemble à ceci\n",
    "```\n",
    "20090505-000000 af Spesiaal:Onlangse_wysigings 3 101681\n",
    "20090505-000000 af Spesiaal:RecentChanges 2 2248\n",
    "20090505-000000 af Suid-Afrika 1 30698\n",
    "20090505-000000 af Tuisblad 14 155257 \n",
    "20090505-000000 af Varkgriep 4 42236\n",
    "20090505-000000 af Wikipedia 2 32796\n",
    "```\n",
    "\n",
    "Il s'agit d'un fichier tabulaire, où chaque ligne est une entrée distincte et les colonnes représentent\n",
    "1. la date et l'heure d'échantillonnage\n",
    "2. la langue\n",
    "3. le titre de la page\n",
    "4. le nombre de visionnement de la page\n",
    "5. la taille de la page en octet.\n",
    "\n",
    "On peut visualier quelques entrées en utilisant la méthode d'action `take` du RDD pour obtenir les K premiers éléments d'un jeu de données. Ici `K = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pagecounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puisque `take` retourne une liste, on peut itérer sur le résultat et l'afficher de manière plus lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in pagecounts.take(10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir de l'aide\n",
    "\n",
    "À tout moment, vous pouvez obtenir de l'aide sur tout objet Python en appelant la fonction `help`. Par exemple, si on voulait en apprendre plus sur la méthode `take` du RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(pagecounts.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action sur un jeu de données\n",
    "\n",
    "La méthode `take` n'est qu'une parmi plusieurs *actions* que l'on peut effectuer sur un RDD. Une liste exhaustive des actions est disponible à l'adresse suivante:\n",
    "https://spark.apache.org/docs/latest/programming-guide.html#actions\n",
    "\n",
    "Si on ne veut pas quitter le notebook, on peut appeler directement `help` sur le RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(pagecounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons par exemple la méthode `count` qui retourne le nombre d'éléments dans un RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque action commise sur un RDD entraîne la création d'une ou de plusieurs tâches et la production d'un résultat. Toutes les tâches réalisées dans un même contexte Spark peuvent être visualisées dans la console web de Spark: [http://<**hostname**>:4040/](http://hostname:4040/)\n",
    "\n",
    "À partir de cette interface, on peut suivre la progression d'une tâche, et consulter diverses mesures sur l'exécution de la tâche, par exemple sa durée et les statistiques de cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation d'un jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on reprend les 10 premiers éléments de notre jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first10 = pagecounts.take(10)\n",
    "first10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que le RDD est formé de chacune des lignes de notre fichier d'entrées, mais qu'il nous est impossible d'accéder aux colonnes en tant que telles. **Pourquoi?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc devoir transformer ce premier RDD en un second de manière à ce que chaque chaîne de caractères soit divisées en une liste de cing éléments. Pour ce faire, on utilise la fonction `string.split`. \n",
    "\n",
    "Importons d'abord le module string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquons ensuite la fonction `string.split` sur le premier élément:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string.split(pagecounts.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant appliquer cette transformation à tous les éléments du RDD. La méthode `map` applique à chaque élément d'un RDD une fonction fournie en argument et retourne un nouveau RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tab = pagecounts.map(string.split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'évaluation d'une transformation sur un RDD est dite _lazy_ ou paresseuse. Spark n'effectue aucun travail tant que le contenu du RDD n'est pas sollicité par une action. Pour vous en convaincre, visitez la [console web Spark](http://<hostname>:4040/jobs/), appliquez un map, puis retournez à la console.\n",
    "\n",
    "Vous constaterez qu'aucun \"job\" ne s'est ajouté à la liste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en cache d'un RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'on s'attend à effectuer plusieurs opérations sur un même jeu de données, il peut être utile de spécifier à Spark de le garder en mémoire.\n",
    "\n",
    "Pour ce faire on utilise la méthode `cache`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tab.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données n'est transféré en mémoire que lorsqu'une action est appliquée. Les RDD stockés en mémoire peuvent être visualisés dans la section **Storage** de l'interface web de Spark.\n",
    "\n",
    "Pour libérer l'espace mémoire prise par un RDD en cache dont on n'aurait plus besoin, on appelle la méthode `unpersist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tab.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrer un RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on dispose maintenant d'un nouveau RDD plus facile à manipuler, on peut débuter l'analyse. Intéressons nous d'abord aux pages en langue anglaise.\n",
    "\n",
    "La ligne suivante filtre le dernier RDD que nous avons créé et ne conserve que les entrées en anglais.\n",
    "* Quel genre d'argument prend la fonction `filter`?\n",
    "* Qu'est-ce que signifie le mot clé **`lambda`**?\n",
    "* Est-ce que `filter` retourne un RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pagecounts_en = pagecounts_tab.filter(lambda list_: list_[1] == \"en\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant compter le nombre de pages en anglais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pagecounts_en.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque qu'on a indiqué à Spark de conserver en mémoire ce nouveau jeu de données, le temps nécessaire pour effectuer le décompte du nombre de pages devrait être plus court lors de la deuxième exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time pagecounts_en.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous avons finalement appliqué une action sur un RDD en cache, ce dernier devrait maintenant être figuré dans l'interface **Storage** de la [console Spark](http://<hostname>:4040/storage/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opération de réduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant à faire un diagramme à bande du nombre de pages vues dans chacune des langues de notre jeu de données. Pour ce faire, nous allons devoir procéder à une transformation de type _réduction_ de notre jeu de données.\n",
    "\n",
    "Dans un premier temps, on transforme notre jeu de données pour conserver seulement la langue et le nombre de vues. Le nombre de vue étant une chaîne de caractères, nous utilisons la fonction `int` pour obtenir convertir la chaîne en entier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tuple = pagecounts_tab.map(lambda entry: (entry[1], int(entry[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualiser les 5 premiers éléments de ce nouveau RDD pour confirmer qu'il s'agit du bon format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tuple.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformation que nous avons effectuée permet à Spark de reconnaître notre RDD comme un ensemble de paires clé-valeur. On peut donc maintenant utiliser les fonctions basées sur ce type structure. Par exemple, on peut créer un nouveau RDD contenant seulement les clés, soit les langues en appelant la méthode `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts_tuple.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut calculer le nombre total de page s ue s our chaque langue. Pour ce faire, on utilise la fonction `reduceByKey`. Cette fonction s'attend à ce que chaque entrée du RDD soit structuré comme des paires clé-valeur. Dans notre cas, la clé est la langue et la valeur est le nombre de vue. \n",
    "\n",
    "`reduceByKey` va combiner les valeurs pour chacune des clés en utilisant une fonction de réduction associative.\n",
    "La méthode prend en argument une fonction prenant en argument deux valeurs et retournant le reéultat de leur combinaison. Dans notre cas, on effectue la somme des valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang_pagecounts = pagecounts_tuple.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque la réduction est une transformation, le résultat est un nouveau RDD.\n",
    "\n",
    "Pour visualiser la totalité du contenu de ce dernier RDD, on peut appeler la fonction `collect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang_pagecounts_local = lang_pagecounts.collect()\n",
    "print(lang_pagecounts_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir seulement les 5 langues les plus populaires, il faut effectuer un tri. Plusieurs choix s'offrent à nous:\n",
    "\n",
    "1- Effectuer le tri localement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5 = sorted(lang_pagecounts_local, key=lambda x: x[1], reverse=True)[:5]\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Utiliser Spark pour effectuer un tri distribué en utilisant la fonction `sortByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5 = lang_pagecounts.map(lambda x: (x[1], x[0]))\\\n",
    "                      .sortByKey(False)\\\n",
    "                      .map(lambda x: (x[1], x[0]))\\\n",
    "                      .take(5)\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Utiliser la méthode `top` du RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5 = lang_pagecounts.top(5, lambda x: x[1])\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite créer un diagramme à bandes des 5 langues les plus populaires de notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "top5_t = zip(*top5)\n",
    "ax.bar(range(len(top5_t[0])), top5_t[1], width=0.35, align=\"center\")\n",
    "ax.set_xticks(range(len(top5_t[0])))\n",
    "ax.set_xticklabels(top5_t[0])\n",
    "ax.set_xlabel(\"langue\")\n",
    "ax.set_ylabel(u\"décompte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminer l'analyse\n",
    "\n",
    "Une fois l'analyse terminée, on doit mettre fin au contexte Spark à l'aide de la méthode `stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
