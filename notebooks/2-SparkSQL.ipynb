{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de données structurées avec SparkSQL\n",
    "\n",
    "Pour éviter d'avoir à constamment manipuler des jeux de données au format texte, il peut être intéressant de structurer nos données. SparkSQL permet de structurer un jeu de données en définissant un schéma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- **EXERCICE** Commencez d'abord par importer le module Python pour Spark et créez un contexte. **Attention**, vérifiez s'il existe déjà un contexte Spark ou gérez les exceptions en conséquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- On importe ensuite les composantes dont on a besoin du module SparkSQL:  \n",
    "- `SQLContext`: contexte qui va nous permettre d'interroger nos données avec des commandes SQL\n",
    "- `Row`: classe qui va nous permettre de définir le schéma de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3- **EXERCICE** On va créer un nouveau RDD à partir du jeu de données de l'introduction soit `data/pagecounts`. Créez un nouveau RDD à partir de `data/pagecounts`. Transformez ensuite le RDD en un second nommé `parts` où chaque ligne est subdivisée en une liste de 4 éléments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1- **EXERCICE** Pour vérifier que le résultat est correct, comptez le nombre d'éléments dans le RDD `parts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2- **EXERCICE** Affichez les 8 premiers éléments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- On peut maintenant structurer nos données. Pour ce faire, on va transformer chaque liste dans notre RDD `parts` en un objet `Row`. La structure nous permet de convertir une chaîne de caractères en un type (un entier par exemple) et de nommer les champs de notre structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecounts = parts.map(lambda p: Row(lang=p[0], name=p[1], pagecount=long(p[2]), size=long(p[3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1- **EXERCICE** Afficher l'attribut `name` du premier élément du RDD `pagecounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- À partir du RDD `pagecounts` où chaque élément est un objet de type `Row`, on peut créer un nouveau RDD de type `DataFrame`. En plus de permettre les opérations standards des RDD, un `DataFrame` contient certaines informations supplémentaires tel que le nom et le type des colonnes du jeu de données.\n",
    "\n",
    "On commence donc par créer un `DataFrame` à partir d'un RDD existant. La fonction `createDataFrame` détermine automatiquement la structure que devra avoir notre RDD, soit le nom et le type des colonnes, à partir des objets de type `Row`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts = sqlCtx.createDataFrame(pagecounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation de la classe `Row` nous permet de spécifier le type de chacune des colonnes. Si on avait simplement créer un dataframe à partir de `parts`, voici ce qu'on aurait observé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.createDataFrame(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7- On enregistre ensuite notre schéma comme une table qui pourra être interrogée à partir de requêtes SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.registerTempTable(\"page_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8- On peut maintenant interroger notre jeu de données à l'aide d'une requête SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_spanish = sqlCtx.sql(\"SELECT name, pagecount \"\n",
    "                         \"FROM page_table \"\n",
    "                         \"WHERE pagecount>=100 \"\n",
    "                         \"AND lang='es' \"\n",
    "                         \"ORDER BY pagecount DESC \"\n",
    "                         \"LIMIT 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décomposons la requête en composantes fonctionelles\n",
    "### SELECT\n",
    "\n",
    "Indique les variables que l'on veut collecter. Les nom des variables ont été définis lors de la création des objets `Row`.\n",
    "\n",
    "### FROM\n",
    "\n",
    "Indique la source des données. Le nom de la table a été défini lors de l'enregistrement à l'étape 7.\n",
    "\n",
    "### WHERE\n",
    "\n",
    "Filtre les entrées selon certaines caractériques. \n",
    "\n",
    "### ORDER BY [...] DESC\n",
    "\n",
    "Indique que l'on veut ordonner nos résultats en fonction de l'une des variables. DESC indique qu'on veut que les données soit ordonnées de manière décroissante. \n",
    "\n",
    "### LIMIT N \n",
    "\n",
    "Conserve que les N premiers entrées de notre requête.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- **EXERCICE** Comme pour toute méthode de transformation d'un RDD, l'évaluation de la commande ne s'effectue que lorsqu'un résultat est exigé par une méthode d'action. Entrez la commande d'action permettant de récuperer la totalité des résultats de notre RDD `top_spanish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_spanish.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque cependant que certaines page reviennent plusieurs fois dans notre palmarès. La raison est qu'on a omis d'additionner le nombre de vues pour une même page. Il faut effectuer une opération d'aggrégation `GROUP` et `SUM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.sql(\"SELECT name, SUM(pagecount) as sumation \"\n",
    "           \"FROM page_table \"\n",
    "           \"WHERE pagecount>=100 \"\n",
    "           \"AND lang='es' \"\n",
    "           \"GROUP BY name \"\n",
    "           \"ORDER BY sumation DESC \"\n",
    "           \"LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- **EXERCICE** Pour vous convaincre de l'utilité de SparkSQL pour simplifier l'analyse de données, écrivez le code nécessaire en utilisant les méthode de transformation des RDD de base (`map`, `filter`, `reduce`, etc.) et d'action (`first`, `collect`, `take`) pour produire le même résultat que la requête SQL précédente.\n",
    "\n",
    "Utilisez le RDD que vous avez créez au début du notebook comme point de départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "11- Pour éviter d'avoir à restructer nos données à chaque fois, on peut sauvegarder les au format [Apache Parquet](https://parquet.apache.org/). Le format va conserver le schéma et l'ordre des données intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfPageCounts.write.parquet(\"data/pagecounts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12- On peut ensuite facilement créer un nouveau `DataFrame` en lisant nos fichiers au format Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pagecount_parq = sqlCtx.read.parquet(\"data/pagecounts.parquet\")\n",
    "pagecount_parq.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13- Finalement, on arrête le contexte Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations supplémentaires\n",
    "\n",
    "- On peut connecter Spark SQL sur une base de données externes, par exemple une base de données PostgreSQL.\n",
    "- Le `Dataframe` permet d'obtenir le même niveau de performance peu importe le langage utilisée. Les RDD classiques sont normalement plus performant en Java ou en Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice récapitulatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Créez un nouveu notebook.\n",
    "1. Créez un nouveau contexte Spark et un contexte SQL.\n",
    "1. Créez un RDD à partir des données d'entrée `data/pagecounts.parquet`.\n",
    "2. Transformez le RDD en un RDD contenant la taille totale des pages vues par langue  \n",
    "    1. À l'aide des méthodes Spark  \n",
    "    2. À l'aide d'une requête SQL\n",
    "3. Limitez le contenu du RDD au 3 langues les moins populaires \n",
    "4. Affichez le résultat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
